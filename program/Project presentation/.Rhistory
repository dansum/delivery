symbol(goog)
5^5
for i = 1 to 10; print i;
exit
close
exit r
close r
escape
5^5
getwd()
setwd(C:\Archive\Work-Related\R_Training\Temp)
3+4
5+6/7
save.image("~/.RData")
save.image("C:/Archive/Technical/R/STMD_Workspace_for_R_201110.RData")
save.image("~/.RData")
ls()
setwd("C:\Archive\Dropbox\Dropbox\PersWork\ML_Class\Competition\Submit\")
help.start()
q()
source('~/.active-rstudio-document')
install.packages('shiny')
library('shiny')
shinyUI(bootstrapPage(
selectInput(inputId = "n_breaks",
label = "Number of bins in histogram (approximate):",
choices = c(10, 20, 35, 50),
selected = 20),
checkboxInput(inputId = "individual_obs",
label = strong("Show individual observations"),
value = FALSE),
checkboxInput(inputId = "density",
label = strong("Show density estimate"),
value = FALSE),
plotOutput(outputId = "main_plot", height = "300px"),
# Display this only if the density is shown
conditionalPanel(condition = "input.density == true",
sliderInput(inputId = "bw_adjust",
label = "Bandwidth adjustment:",
min = 0.2, max = 2, value = 1, step = 0.2)
)
))
install.packages("RSQLite")
?RSQLite
??RSQLite
??ff
require("ff")
library("ff")
install.package("ff")
install.packages("ff")
library("ff")
??ff
?ff
install.packages("bigmemory")
?bigmemory
??bigmemory
install.packages("bigmemory")
# 1. Set up environment
require("RJSONIO") || install.packages("RJSONIO")
require("stringr") || install.packages("stringr")  # TBD if this is necessary
require("RCurl")   || install.packages("RCurl")
# 2. Load the input data of Youtube IDs
if (!exists("youtubeID")) {
#  2.1. For list of Bulgarian subtitles:
url <- "https://docs.google.com/spreadsheet/pub?key=0AjbLIL-lySvRdFJpMTh4TUpORzZubDl0YXlsejZ2QXc&single=true&gid=19&output=csv"
#   Then, "Import Dataset" >> "From Web URL..."
youtubeID.online <- read.csv(url,skip=3,stringsAsFactors = FALSE) # # fileEncoding = "windows-1251", encoding = "UTF8"
#   Select only necessary data: all cells under the one containing "Българскo ID"
#     Maybe also retain the categorization (eg, Math)
youtubeID.online.bg <- youtubeID.online[,9]
youtubeID <- youtubeID.online.bg[!(youtubeID.online.bg == "")]
if(length(youtubeID) != length(unique(youtubeID))) {check.unique <- "Error"}
}
View(youtubeID.online)
# 3. Call the API
api.root <- "https://www.googleapis.com/youtube/v3/videos?part=contentDetails%2Cstatistics&id="
api.key <- "AIzaSyDPd2LR8_WBO3aJEciFkR98RXFl3-Zq4i0"   # Provide credentials
api.list <- paste(api.root, youtubeID, "&key=", api.key,sep="")   # Prepare the list of URIs
api.response <- getURL(api.list,ssl.verifypeer = FALSE)   # Call the API
# 4. Prepare for extracting the data
#     stats.names <- c("duration","viewCount","likeCount","dislikeCount","favoriteCount","commentCount")
api.success <- grepl("\"duration\"",api.response)   # Find good rows
api.clean <- api.response[api.success]
obs <- length(api.clean)
stats <- data.frame(id=character(obs),duration=integer(obs),view=integer(obs),like=integer(obs),dislike=integer(obs),favorite=integer(obs),comment=integer(obs),date=Sys.Date())  # Prepate the format for the final answer
stats[,"id"] <- youtubeID[api.success]
#   4.1. Extract duration
matches <- regexec("duration\": \"PT([0-9A-Z]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"duration"] <- sapply(matches,function(x) x[2])
#     4.1. Fix stats.duration from its mmMssS format
#   4.2. Extract viewCount
matches <- regexec("\"viewCount\": \"([0-9]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"view"] <- sapply(matches,function(x) x[2])
#   4.3. Extract likeCount
matches <- regexec("\"likeCount\": \"([0-9]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"like"] <- sapply(matches,function(x) x[2])
#   4.4. Extract dislikeCount
matches <- regexec("\"dislikeCount\": \"([0-9]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"dislike"] <- sapply(matches,function(x) x[2])
#   4.5. Extract favoriteCount
matches <- regexec("\"favoriteCount\": \"([0-9]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"favorite"] <- sapply(matches,function(x) x[2])
#   4.6. Extract commentCount
matches <- regexec("\"commentCount\": \"([0-9]*)\"\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"comment"] <- sapply(matches,function(x) x[2])
# ages <- as.numeric(agescharacter)
# 5. Output the results
write.csv(stats, file=paste("Khan_viewstats_",Sys.Date(),".csv",sep=""))
getwd()
# 1. Set up environment
source.with.encoding('C:/Archive/Data/Dropbox/R/Khan/getYTstat_20140401.R', encoding='UTF-8')
require("RJSONIO") || install.packages("RJSONIO")
require("stringr") || install.packages("stringr")  # TBD if this is necessary
require("RCurl")   || install.packages("RCurl")
url <- "https://docs.google.com/spreadsheet/pub?key=0AjbLIL-lySvRdFJpMTh4TUpORzZubDl0YXlsejZ2QXc&single=true&gid=19&output=csv"
youtubeID.online <- read.csv(url,skip=3,stringsAsFactors = FALSE) # # fileEncoding = "windows-1251", encoding = "UTF8"
youtubeID.online.bg <- youtubeID.online[,9]
youtubeID <- youtubeID.online.bg[!(youtubeID.online.bg == "")]
if(length(youtubeID) != length(unique(youtubeID))) {check.unique <- "Error"}
api.root <- "https://www.googleapis.com/youtube/v3/videos?part=contentDetails%2Cstatistics&id="
api.key <- "AIzaSyDPd2LR8_WBO3aJEciFkR98RXFl3-Zq4i0"   # Provide credentials
api.list <- paste(api.root, youtubeID, "&key=", api.key,sep="")   # Prepare the list of URIs
api.response <- getURL(api.list,ssl.verifypeer = FALSE)   # Call the API
api.success <- grepl("\"duration\"",api.response)   # Find good rows
api.clean <- api.response[api.success]
obs <- length(api.clean)
stats <- data.frame(id=character(obs),duration=integer(obs),view=integer(obs),like=integer(obs),dislike=integer(obs),favorite=integer(obs),comment=integer(obs),date=Sys.Date())  # Prepate the format for the final answer
stats[,"id"] <- youtubeID[api.success]
#   4.1. Extract duration
matches <- regexec("duration\": \"PT([0-9A-Z]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"duration"] <- sapply(matches,function(x) x[2])
#     4.1. Fix stats.duration from its mmMssS format
#   4.2. Extract viewCount
matches <- regexec("\"viewCount\": \"([0-9]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"view"] <- sapply(matches,function(x) x[2])
#   4.3. Extract likeCount
matches <- regexec("\"likeCount\": \"([0-9]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"like"] <- sapply(matches,function(x) x[2])
#   4.4. Extract dislikeCount
matches <- regexec("\"dislikeCount\": \"([0-9]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"dislike"] <- sapply(matches,function(x) x[2])
#   4.5. Extract favoriteCount
matches <- regexec("\"favoriteCount\": \"([0-9]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"favorite"] <- sapply(matches,function(x) x[2])
#   4.6. Extract commentCount
matches <- regexec("\"commentCount\": \"([0-9]*)\"\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"comment"] <- sapply(matches,function(x) x[2])
# ages <- as.numeric(agescharacter)
# 5. Output the results
write.csv(stats, file=paste("Khan_viewstats_",Sys.Date(),".csv",sep=""))
?write.csv
temp <- "C:\\Archive\\Data\\Dropbox\\Work\\Volvo\\Data\\Dictionary\\Web_Social\\Celebrus_201401\\ns_click_fact_session_number_20140418.csv"
temp <- "C:\\Archive\\Data\\Dropbox\\Work\\Volvo\\Data\\Dictionary\\Web_Social\\Celebrus_201401\\ns_click_fact_session_number_20140418.csv"
temp <- read.csv(temp)
temp2 <- options()
?read.csv
options(stringsAsFactors = FALSE)
rm(temp2)
View(temp)
i <- 1000000
i <- 1000000
temp2 <- cbind(temp[1:i,],temp[i+1:2*i,],temp[2i+1:3*i,],temp[3i+1:4*i,],temp[4i+1:nrow(temp),])
temp2 <- cbind(temp[1:i,],
temp[(i+1):(2*i),],
temp[(2i+1):(3*i),],
temp[(3i+1):(4*i),],
temp[(4i+1):nrow(temp),]
)
temp2 <- cbind(temp[1:i,1],
temp[(i+1):(2*i),1],
temp[(2i+1):(3*i),1],
temp[(3i+1):(4*i),1],
temp[(4i+1):nrow(temp),1]
)
nrow(temp)
temp2 <- cbind(temp[1:1000000,1],
temp[1000001:2000000,1],
temp[2000001:3000000,1],
temp[3000001:4000000,1],
temp[4000001:nrow(temp),1]
)
View(temp2)
?write.csv()
address <- "C:\\Archive\\Data\\Dropbox\\Work\\Volvo\\Data\\Dictionary\\Web_Social\\Celebrus_201401\\ns_click_fact_session_number_20140418_reshaped.csv"
write.csv(temp2,address,row.names = FALSE)
count(unique(temp))
length
length(unique(temp))
temp3 <- unique(temp)
nrow(temp3 <- unique(temp))
write.csv(temp3,address,row.names = FALSE)
getwd()
require("RJSONIO") || install.packages("RJSONIO")
require("stringr") || install.packages("stringr")  # TBD if this is necessary
require("RCurl")   || install.packages("RCurl")
if (!exists("youtubeID")) {
#  2.1. For list of Bulgarian subtitles:
url <- "https://docs.google.com/spreadsheet/pub?key=0AjbLIL-lySvRdFJpMTh4TUpORzZubDl0YXlsejZ2QXc&single=true&gid=19&output=csv"
#   Then, "Import Dataset" >> "From Web URL..."
youtubeID.online <- read.csv(url,skip=3,stringsAsFactors = FALSE) # # fileEncoding = "windows-1251", encoding = "UTF8"
#   Select only necessary data: all cells under the one containing "Българскo ID"
#     Maybe also retain the categorization (eg, Math)
youtubeID.online.bg <- youtubeID.online[,9]
youtubeID <- youtubeID.online.bg[!(youtubeID.online.bg == "")]
if(length(youtubeID) != length(unique(youtubeID))) {check.unique <- "Error"}
}
api.root <- "https://www.googleapis.com/youtube/v3/videos?part=contentDetails%2Cstatistics&id="
api.key <- "AIzaSyDPd2LR8_WBO3aJEciFkR98RXFl3-Zq4i0"   # Provide credentials
api.list <- paste(api.root, youtubeID, "&key=", api.key,sep="")   # Prepare the list of URIs
api.response <- getURL(api.list,ssl.verifypeer = FALSE)   # Call the API
api.success <- grepl("\"duration\"",api.response)   # Find good rows
api.clean <- api.response[api.success]
sum(api.success)
api.clean <- api.response[api.success]
obs <- length(api.clean)
stats <- data.frame(id=character(obs),duration=integer(obs),view=integer(obs),like=integer(obs),dislike=integer(obs),favorite=integer(obs),comment=integer(obs),date=Sys.Date())  # Prepate the format for the final answer
stats[,"id"] <- youtubeID[api.success]
matches <- regexec("duration\": \"PT([0-9A-Z]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"duration"] <- sapply(matches,function(x) x[2])
matches <- regexec("\"viewCount\": \"([0-9]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"view"] <- sapply(matches,function(x) x[2])
matches <- regexec("\"likeCount\": \"([0-9]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"like"] <- sapply(matches,function(x) x[2])
matches <- regexec("\"dislikeCount\": \"([0-9]*)\",\n",api.clean)
stats[,"dislike"] <- sapply(matches,function(x) x[2])
matches <- regmatches(api.clean,matches)
matches <- regexec("\"favoriteCount\": \"([0-9]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"favorite"] <- sapply(matches,function(x) x[2])
matches <- regexec("\"commentCount\": \"([0-9]*)\"\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"comment"] <- sapply(matches,function(x) x[2])
write.csv(stats, file=paste("Khan_viewstats_",Sys.Date(),".csv",sep="",row.names=FALSE))
write.csv(stats, file=paste("Khan_viewstats_",Sys.Date(),".csv",sep=""),row.names=FALSE)
grep("jun","jung")
grep("jun","dsadsajung")
?grep
grep("jun",c("jung","jug","dssajung","juju"))
grepl("jun",c("jung","jug","dssajung","juju"))
require("reshape") || install.packages("reshape")
test<- c(c(a,a,3),c(a,b,5),c(a,a,9))
test<- c(c("a","a",3),c("a","b",5),c("a","a",9))
test<- data.frame(c("a","a",3),c("a","b",5),c("a","a",9))
View(test)
test<- data.frame(c("a","a","b"),c("a","b","a"),c(3,5,9))
View(test)
colnames(test)<-c("var1","var2","var3")
View(test)
cast (test, var1 ~ var2)
cast (test, var1 + var2 ~ .)
View(test)
test<- data.frame(c("a","a","b"),c("a","a","a"),c(3,5,9))
cast (test, var1 ~ var2)
colnames(test)<-c("var1","var2","var3")
cast (test, var1 + var2 ~ .)
cast (test, var1 ~ var2)
View(test)
View(test)
View(test)
# 1. Set up environment
getwd()
require("RJSONIO") || install.packages("RJSONIO")
require("stringr") || install.packages("stringr")  # TBD if this is necessary
require("RCurl")   || install.packages("RCurl")
if (!exists("youtubeID")) {
#  2.1. For list of Bulgarian subtitles:
url <- "https://docs.google.com/spreadsheet/pub?key=0AjbLIL-lySvRdFJpMTh4TUpORzZubDl0YXlsejZ2QXc&single=true&gid=19&output=csv"
youtubeID.online <- read.csv(url,skip=3,stringsAsFactors = FALSE) # # fileEncoding = "windows-1251", encoding = "UTF8"
youtubeID.online.bg <- youtubeID.online[,9]
youtubeID <- youtubeID.online.bg[!(youtubeID.online.bg == "")]
if(length(youtubeID) != length(unique(youtubeID))) {check.unique <- "Error"}
}
api.root <- "https://www.googleapis.com/youtube/v3/videos?part=contentDetails%2Cstatistics&id="
api.key <- "AIzaSyDPd2LR8_WBO3aJEciFkR98RXFl3-Zq4i0"   # Provide credentials
api.list <- paste(api.root, youtubeID, "&key=", api.key,sep="")   # Prepare the list of URIs
api.response <- getURL(api.list,ssl.verifypeer = FALSE)   # Call the API
api.success <- grepl("\"duration\"",api.response)   # Find good rows
api.clean <- api.response[api.success]
obs <- length(api.clean)
stats <- data.frame(id=character(obs),duration=integer(obs),view=integer(obs),like=integer(obs),dislike=integer(obs),favorite=integer(obs),comment=integer(obs),date=Sys.Date())  # Prepate the format for the final answer
stats[,"id"] <- youtubeID[api.success]
matches <- regexec("duration\": \"PT([0-9A-Z]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"duration"] <- sapply(matches,function(x) x[2])
matches <- regexec("\"viewCount\": \"([0-9]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"view"] <- sapply(matches,function(x) x[2])
matches <- regexec("\"likeCount\": \"([0-9]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"like"] <- sapply(matches,function(x) x[2])
matches <- regexec("\"dislikeCount\": \"([0-9]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"dislike"] <- sapply(matches,function(x) x[2])
matches <- regexec("\"favoriteCount\": \"([0-9]*)\",\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"favorite"] <- sapply(matches,function(x) x[2])
matches <- regexec("\"commentCount\": \"([0-9]*)\"\n",api.clean)
matches <- regmatches(api.clean,matches)
stats[,"comment"] <- sapply(matches,function(x) x[2])
write.csv(stats, file=paste("Khan_viewstats_",Sys.Date(),".csv",sep=""),row.names=FALSE)
path <- "C:\\Archive\\TEMP\\TEMP_Analyses\\Volvo_CampaignHistory_HH_20140224\\Volvo_CampaignHistory_HH_20140224.txt"
input <- read.csv(path,stringsAsFactors=FALSE)
head(input)
temp <- input[1:10,]
View(temp)
temp <- input[1:10,1]
temp
temp <- input[1:10,4]
temp <- input[1,4]
temp
options(scipen=999)
temp
temp <- input[1:10,4]
temp
require("reshape") || install.packages("require(reshape)")
campaigns <- cast(input, CAMPAIGN_KEY ~ .)
View(campaigns)
colnames[camapigns]
colnames[campaigns]
colnames(campaigns])
colnames(campaigns)
colnames(campaigns)[2] <- "sum"
colnames(campaigns)
temp <- campaigns[order(sum)]
temp <- campaigns[order(sum),]
write.csv(campaigns,file="tmep",rownames=FALSE)
?write.csv
write.csv(campaigns,file="temp",row.names=FALSE)
path <- "C:\\Archive\\TEMP\\TEMP_Analyses\\Volvo_CampaignHistory_HH_20140224\\Volvo_CampaignHistory_HH_20140224.txt"
input <- read.csv(path,stringsAsFactors=FALSE)
input[,1]
input[1,]
input_NovAWD <- [CAMPAIGN_KEY == "November AWD Offer",]
input_NovAWD <- [[CAMPAIGN_KEY == "November AWD Offer",]]
input_NovAWD <- input[[CAMPAIGN_KEY == "November AWD Offer",]]
input_NovAWD <- input[CAMPAIGN_KEY == "November AWD Offer",]
input_NovAWD <- input[input$CAMPAIGN_KEY == "November AWD Offer",]
write.csv(input_NovAWD,file="temp",row.names=FALSE)
path <- "C:\\Archive\\TEMP\\TEMP_Analyses\\Volvo_CampaignHistory_HH_20140224\\Volvo_CampaignHistory_HH_20140224.txt"
input <- read.csv(path,stringsAsFactors=FALSE)
input_S60AWD <- input[input$CAMPAIGN_KEY == "MY13 Volvo S60 T5 with AWD",]
write.csv(input_S60AWD,file="temp",row.names=FALSE)
?rtools
??rtools
R.version
R.Version()
Sys.getenv("PATH")
system('g++ -v')
library(shinyapps)
shinyapps::setAccountInfo(name='dancho', token='1E59E1E423FF02208755119A7FA857A7', secret='g0upOFX2ti2Rd3axD0igIWcskvF8G6P0Z4Sc07ku')
deployApp()
setwd("C:\\Archive\\Data\\Dropbox\\R\\dataprod_201406\\Project\\program")
deployApp()
install.packages("markdown")
library(markdown)
deployApp()
require(devtools)
install_github('rmarkdown','rstudio')
deployApp()
library(shiny)
install.packages("shiny")
library(shiny)
deployApp()
deployApp()
browseURL('index.html')
setwd("C:\\Archive\\Data\\Dropbox\\R\\dataprod_201406\\Project\\program\\Project presentation")
library(slidify)
author("Project presentation")
setwd("C:\\Archive\\Data\\Dropbox\\R\\dataprod_201406\\Project\\program\\Project presentation")
library(slidify)
library(knitr)
slidify('index.Rmd')
browseURL('index.html')
publish("dansum", "learn")
publish_github("dansum", "learn")
